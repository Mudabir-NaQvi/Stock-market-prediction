{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical data web scraping (Downlaoding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code is generated by Insomnia application \n",
    "* Output of this code is raw HTML Code (tags/data)\n",
    "* Use (payload) variable to assign credencials i.e. month, year <br> and symbol (name of company in short)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!-- .note(style=\"margin-bottom: 1rem;\")div * Queued order represents cummulative volume of all orders placed in the trading system during the day.\n",
      "div ** Cancelled order represents volume of all the cancelled orders excluding the orders cancelled by the trading system during dump state.\n",
      "--><div class=\"tbl__wrapper tbl__wrapper--scrollable\"><table class=\"tbl\" id=\"historicalTable\" data-page-length=\"25\"><thead class=\"tbl__head\"><!-- trth(colspan=\"6\")\n",
      "th.center(colspan=\"2\", style=\"width: 110px;\") TOTAL BUY ORDERS\n",
      "th.center(colspan=\"2\", style=\"width: 110px;\") TOTAL SELL ORDERS--><tr><th data-name=\"time\" data-type=\"string\">TIME</th><th class=\"right\" data-name=\"open\" data-type=\"number\">OPEN</th><th class=\"right\" data-name=\"high\" data-type=\"number\">HIGH</th><th class=\"right\" data-name=\"low\" data-type=\"number\">LOW</th><th class=\"right\" data-name=\"close\" data-type=\"number\">CLOSE</th><th class=\"right\" data-name=\"volume\" data-type=\"number\">VOLUME</th><!-- th.right(data-name=\"obq\", data-type=\"numb\n"
     ]
    }
   ],
   "source": [
    "import http.client\n",
    "conn = http.client.HTTPSConnection(\"dps.psx.com.pk\")\n",
    "payload = \"month=3&year=2022&symbol=OGDC\"\n",
    "headers = {\n",
    "    'Accept': \"text/html, */*; q=0.01\",\n",
    "    'Accept-Language': \"en-US,en;q=0.9\",\n",
    "    'Connection': \"keep-alive\",\n",
    "    'Content-Type': \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "    'Origin': \"https://dps.psx.com.pk\",\n",
    "    'Referer': \"https://dps.psx.com.pk/historical\",\n",
    "    'Sec-Fetch-Dest': \"empty\",\n",
    "    'Sec-Fetch-Mode': \"cors\",\n",
    "    'Sec-Fetch-Site': \"same-origin\",\n",
    "    'Sec-GPC': \"1\",\n",
    "    'User-Agent': '''Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/\n",
    "    537.36(KHTML, like Gecko)Chrome/103.0.5060.114 Safari/537.36,\n",
    "    'X-Requested-With': \"XMLHttpRequest'''\n",
    "    }\n",
    "conn.request(\"POST\", \"/historical\", payload, headers)\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "print(data.decode(\"utf-8\")[:1000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the final altered code for (Web Scraping) downloading Historical Data \n",
    "* Output of this code is CSV files.\n",
    "* All thirty (30) companies' names which we have used in our project are <br> listed on the \"symbols\" list.\n",
    "* A loop will iterate through all companies's names and create a CSV <br> fiels containing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGRO 1 2018 Added to Table!\n",
      "ENGRO 2 2018 Added to Table!\n",
      "ENGRO 3 2018 Added to Table!\n",
      "ENGRO 4 2018 Added to Table!\n",
      "ENGRO 5 2018 Added to Table!\n",
      "ENGRO 6 2018 Added to Table!\n",
      "ENGRO 7 2018 Added to Table!\n"
     ]
    }
   ],
   "source": [
    "import http.client\n",
    "import pandas as pd\n",
    "import time\n",
    "symbols=['ENGRO','LUCK', 'OGDC','FFC','HBL','HUBC','PPL', 'POL',\n",
    "         'EFERT','MCB','UBL','DGKC','PSO','SEARL','MLCF','BAHL',\n",
    "         'MARI','TRG','ATRL','UNITY','SYS','MEBL','GHNI','NML',\n",
    "         'PIOC','CHCC','PAEL','ISL','KAPCO','DAWH']\n",
    "months=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "years=[2018,2019,2020,2021,2022,2023]\n",
    "table_data=\"\"\n",
    "all_in_one_df=pd.DataFrame()\n",
    "\n",
    "conn = http.client.HTTPSConnection(\"dps.psx.com.pk\")\n",
    "headers = {\n",
    "    'Accept': \"text/html, */*; q=0.01\",\n",
    "    'Accept-Language': \"en-US,en;q=0.9\",\n",
    "    'Connection': \"keep-alive\",\n",
    "    'Content-Type': \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "    'Origin': \"https://dps.psx.com.pk\",\n",
    "    'Referer': \"https://dps.psx.com.pk/historical\",\n",
    "    'Sec-Fetch-Dest': \"empty\",\n",
    "    'Sec-Fetch-Mode': \"cors\",\n",
    "    'Sec-Fetch-Site': \"same-origin\",\n",
    "    'Sec-GPC': \"1\",\n",
    "    'User-Agent': '''Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \n",
    "    (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36''',\n",
    "    'X-Requested-With': \"XMLHttpRequest\"\n",
    "    }\n",
    "for symbol in symbols:\n",
    "    time.sleep(1)\n",
    "    table_data=\"\"\n",
    "    for year in years:\n",
    "        time.sleep(1)\n",
    "        for month in months:\n",
    "            time.sleep(2)\n",
    "            payload = f\"month={month}&year={year}&symbol={symbol}\"    \n",
    "            conn.request(\"POST\", \"/historical\", payload, headers)\n",
    "            res = conn.getresponse()\n",
    "            data = res.read()\n",
    "            data=data.decode(\"utf-8\")\n",
    "            if len(data)>600:\n",
    "                table_data=table_data+data[1046:-22]\n",
    "                print(f\"{symbol} {month} {year} Added to Table!\")\n",
    "            else:\n",
    "                print(\"Error 404!\")\n",
    "            final=data[0:1046]+table_data+data[-22:]\n",
    "            df=pd.DataFrame(pd.read_html(final)[0])\n",
    "            df.insert(loc=0, column=\"Symbol\", value=symbol)\n",
    "            all_in_one_df=pd.concat([all_in_one_df,df],ignore_index=True)\n",
    "    final=data[0:1046]+table_data+data[-22:]\n",
    "    df=pd.DataFrame(pd.read_html(final)[0])\n",
    "    path=f\"D:\\Documents\\FYP\\Historical Data (dec)\\{symbol}.csv\"\n",
    "    df.to_csv(path)\n",
    "    path=f\"D:\\Documents\\FYP\\Historical Data (dec)\\{symbol}.csv\"\n",
    "    data=pd.read_csv(path)\n",
    "    data['TIME']=pd.to_datetime(data['TIME'])\n",
    "    data=data.sort_values(by='TIME')\n",
    "    data.rename(columns={\"Unnamed: 0\":'index'},inplace=True)\n",
    "    data.drop(\"index\",axis=1,inplace=True)\n",
    "    path=f\"D:\\Documents\\FYP\\Historical Data (dec)\\{symbol}.csv\"\n",
    "    data.to_csv(path)\n",
    "all_in_one_df.to_csv(\"D:\\Documents\\FYP\\Historical Data (dec)/All in one.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A format sample of downloaded data.\n",
    "* Contains data from January 2018 to December 2022\n",
    "* total of five years\n",
    "* with the exact interval of one (1 day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Unnamed: 0        TIME    OPEN    HIGH     LOW   CLOSE   VOLUME\n",
       " 0          22  2018-01-01  274.00  283.55  270.15  280.30  1429100\n",
       " 1          21  2018-01-02  279.00  283.75  277.50  280.39  1577200\n",
       " 2          20  2018-01-03  279.11  283.73  278.50  281.49  1271800\n",
       " 3          19  2018-01-04  281.49  287.00  280.30  284.83  1103100\n",
       " 4          18  2018-01-05  283.10  288.50  283.10  286.70  1146100,\n",
       "       Unnamed: 0        TIME    OPEN    HIGH     LOW   CLOSE  VOLUME\n",
       " 1234        1221  2022-12-26  259.00  265.40  259.00  264.69  327545\n",
       " 1235        1220  2022-12-27  265.45  265.45  260.25  261.26  164502\n",
       " 1236        1219  2022-12-28  263.00  263.00  253.50  256.18  567995\n",
       " 1237        1218  2022-12-29  257.01  261.98  256.18  260.79  342240\n",
       " 1238        1217  2022-12-30  259.01  264.00  259.01  262.01  296703)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path=f\"D:\\Documents\\FYP\\Historical Data 18-22\\ENGRO.csv\"\n",
    "pd.read_csv(path).head(),pd.read_csv(path).tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily data automation Google drive & Naas server"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This basic code will scrape the data available <br> at \"https://dps.psx.com.pk/timeseries/int/ENGRO\" this URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   status message                     data\n",
       " 0       1          [1673006683, 284.68, 1]\n",
       " 1       1          [1673006593, 284.68, 1]\n",
       " 2       1          [1673006578, 284.68, 0]\n",
       " 3       1          [1673006473, 284.68, 0]\n",
       " 4       1          [1673006428, 284.68, 0],\n",
       "       status message                       data\n",
       " 1168       1           [1672978825, 279.4, 150]\n",
       " 1169       1           [1672978815, 278.95, 10]\n",
       " 1170       1           [1672978759, 278.03, 10]\n",
       " 1171       1           [1672978637, 276.98, 50]\n",
       " 1172       1          [1672978501, 279.4, 1000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "response= urllib.request.urlopen(\"https://dps.psx.com.pk/timeseries/int/ENGRO\").read()\n",
    "data=pd.DataFrame(eval(response))\n",
    "data.head(),data.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the final altered code for web scarping (downloading) Daily Data \n",
    "* this code will download data from the given URL and create a CSV \n",
    "* that CSV will be send and stored in the given Google Drive <br> account (you have to provide Credentials and token of the G.Frive account)\n",
    "* to automate this code, uncomment the commented code and run <br> this script on the naas.ai server and script will be scheduled for every 5 day of weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGRO 2022-12-16.csv Downloaded\n",
      "ENGRO 2022-12-16.csv backed up\n",
      "--------------------------------------------------\n",
      "LUCK 2022-12-16.csv Downloaded\n",
      "LUCK 2022-12-16.csv backed up\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "# import naas\n",
    "import os.path\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "scps=[\"https://www.googleapis.com/auth/drive\"]\n",
    "creds= None\n",
    "folders={}\n",
    "list_companies=['ENGRO','LUCK', 'OGDC','FFC','HBL','HUBC',\n",
    "                'PPL', 'POL','EFERT', 'MCB','UBL','DGKC','PSO',\n",
    "                'SEARL','MLCF','BAHL','MARI','TRG','ATRL','UNITY',\n",
    "                'SYS','MEBL','GHNI','NML','PIOC','CHCC','PAEL','ISL',\n",
    "                'KAPCO','DAWH']\n",
    "\n",
    "if os.path.exists(\"token.json\"):\n",
    "    creds=Credentials.from_authorized_user_file(\"token.json\",scps)\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        flow= InstalledAppFlow.from_client_secrets_file(\"client_secret.json\",scps)\n",
    "        creds= flow.run_local_server(port=0)\n",
    "    with open(\"token.json\",'w')as token:\n",
    "        token.write(creds.to_json())\n",
    "try:\n",
    "    service= build(\"drive\",\"v3\",credentials=creds)\n",
    "    response=service.files().list(\n",
    "        q=\"name='FYP PSX daily data' and mimeType='application/vnd.google-apps.folder'\",\n",
    "        spaces='drive'\n",
    "    ).execute()\n",
    "    if not response['files']:\n",
    "        file_metadata={\n",
    "            \"name\":\"FYP PSX daily data\",\n",
    "            \"mimeType\":\"application/vnd.google-apps.folder\",\n",
    "        }\n",
    "        file = service.files().create(body=file_metadata,fields=\"id\").execute()\n",
    "        parent_folder_id=file.get('id')\n",
    "    else:\n",
    "        parent_folder_id=response['files'][0]['id']\n",
    "    for item in list_companies:\n",
    "        time.sleep(2)\n",
    "        service= build(\"drive\",\"v3\",credentials=creds)\n",
    "        response=service.files().list(\n",
    "            q=f\"name='{item}' and mimeType='application/vnd.google-apps.folder'\",\n",
    "            spaces='drive'\n",
    "        ).execute()\n",
    "        if not response['files']:\n",
    "            file_metadata={\n",
    "                \"name\":item,\n",
    "                \"mimeType\":\"application/vnd.google-apps.folder\",\n",
    "                \"parents\":[parent_folder_id]\n",
    "            }\n",
    "            file = service.files().create(body=file_metadata,fields=\"id\").execute()\n",
    "            folder_id=file.get('id')\n",
    "            folders[item]=folder_id\n",
    "        else:\n",
    "            folder_id=response['files'][0]['id']\n",
    "        cnt= urllib.request.urlopen(\"https://dps.psx.com.pk/timeseries/int/\"+item).read()\n",
    "        df=pd.DataFrame(eval(cnt))\n",
    "        loc= f\"Daily data/{item}/{item} {time.strftime('%Y-%m-%d')}.csv\"\n",
    "        name= f\"{item} {time.strftime('%Y-%m-%d')}.csv\"\n",
    "        mypath =\"Daily data/\"+item \n",
    "        if not os.path.isdir(mypath):\n",
    "            os.makedirs(mypath)\n",
    "        df.to_csv(loc)\n",
    "        print(name,\" Downloaded\")\n",
    "        file_metadata={\n",
    "            \"name\": name,\n",
    "            \"parents\":[folder_id]\n",
    "        }\n",
    "        media= MediaFileUpload(loc)\n",
    "        upload_file= service.files().create(body=file_metadata, media_body=media,fields=\"id\").execute()\n",
    "        print(name,\" backed up\")\n",
    "        subject=\"Data stored Successfully!\"\n",
    "        content='''The daily PSX data has been saved successfully to your google drive. Thank you!'''\n",
    "        print(\"-\"*50)\n",
    "except HttpError as e:\n",
    "    print(\"Error\"+str(e))\n",
    "    subject=\"Failled to store data!\"\n",
    "    content='''Data could not be saved to your google drive. please try to download manually!'''\n",
    "email_to=\"email_address\"\n",
    "# naas.notification.send(email_to,subject,content)\n",
    "# naas.scheduler.add(recurrence=\"0 17 * * 1,2,3,4,5\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A format sample of downloaded data.\n",
    "* contains a lot of usable data \n",
    "* 'data' column is the only column of intrest \n",
    "* data column contains timeseries ,value and volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Unnamed: 0  status  message                      data\n",
       " 0           0       1      NaN   [1671621199, 260.11, 1]\n",
       " 1           1       1      NaN  [1671620998, 260.11, 20]\n",
       " 2           2       1      NaN   [1671620822, 260.11, 2]\n",
       " 3           3       1      NaN  [1671620462, 260.11, 50]\n",
       " 4           4       1      NaN  [1671620438, 260.11, 40],\n",
       "    Unnamed: 0  status  message                      data\n",
       " 0           0       1      NaN   [1671621199, 260.11, 1]\n",
       " 1           1       1      NaN  [1671620998, 260.11, 20]\n",
       " 2           2       1      NaN   [1671620822, 260.11, 2]\n",
       " 3           3       1      NaN  [1671620462, 260.11, 50]\n",
       " 4           4       1      NaN  [1671620438, 260.11, 40])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path=f\"D:\\Documents\\FYP\\FYP PSX daily data New\\ENGRO\\ENGRO 2022-12-21.csv\"\n",
    "pd.read_csv(path).head(),pd.read_csv(path).head().tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily data Cleaning and feature extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleanig\n",
    "* this code will extract the usefull information from the downloaded data and create a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "symbols = ['ENGRO', 'LUCK', 'OGDC', 'FFC', 'HBL', 'HUBC','PPL',\n",
    "           'POL', 'EFERT', 'MCB', 'DGKC', 'PSO', 'SEARL', 'MLCF',\n",
    "           'BAHL', 'MARI', 'ATRL', 'UNITY','SYS', 'MEBL', 'GHNI',\n",
    "           'PIOC', 'CHCC', 'PAEL', 'ISL','KAPCO']\n",
    "\n",
    "\n",
    "for symbol in symbols:\n",
    "    path = \"FYP PSX daily data New\"\n",
    "    data=pd.read_csv(f\"FYP PSX daily data New/{symbol}/{symbol} 2022-12-12.csv\")\n",
    "    for item in os.listdir(f\"{path}/{symbol}\"):\n",
    "        df=pd.read_csv(f\"{path}/{symbol}/{item}\")\n",
    "        data=pd.concat([data,df],ignore_index=True)\n",
    "    data=data[\"data\"]\n",
    "    print(\"Rows before cleanup = \", data.shape)\n",
    "    data=data.str.split(',',expand=True)\n",
    "    data.rename(columns={0:'Date',1:'Value',2:\"Volume\"},inplace=True)\n",
    "    temp=[]\n",
    "    for i in data[\"Date\"]:\n",
    "        temp.append(i[1:])\n",
    "    data['Date']=temp\n",
    "    data.drop_duplicates(subset='Date',keep='first',inplace=True)\n",
    "    data[\"Date\"]=pd.to_datetime(data['Date'],unit='s')\n",
    "    data.drop(\"Volume\",axis=1,inplace=True)\n",
    "    path=(f\"Daily Data Cleaned New (Hours)/{symbol}.csv\")\n",
    "    data.to_csv(path)\n",
    "    data=pd.read_csv(path,index_col=False)\n",
    "    data=data.sort_values(by='Date')\n",
    "    print(\"Shape Before \",data.shape)\n",
    "    datelist=data['Date'].tolist()\n",
    "    for i,x in enumerate(datelist):\n",
    "        datelist[i]=datelist[i][:-6]\n",
    "    data['Date']=datelist\n",
    "    data.drop_duplicates(subset='Date',keep='first',inplace=True)\n",
    "    data.rename(columns={\"Unnamed: 0\":'index'},inplace=True)\n",
    "    data.drop(\"index\",axis=1,inplace=True)\n",
    "    print(\"Shape After \",data.shape)\n",
    "    path=f\"Daily Data Cleaned New (Hours)/{symbol}.csv\"\n",
    "    data.to_csv(path)\n",
    "    print(symbol+\" Cleaned and Stored Successfully.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A format sample of downloaded data.\n",
    "* final format of useful data\n",
    "* contains data with the exact interval of an hour\n",
    "* eight (8) working Hours per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Unnamed: 0           Date  Value\n",
       " 0         322  2022-12-12 04  263.0\n",
       " 1         293  2022-12-12 05  262.2\n",
       " 2         257  2022-12-12 06  261.1\n",
       " 3         199  2022-12-12 07  260.9\n",
       " 4         149  2022-12-12 08  260.5,\n",
       "     Unnamed: 0           Date   Value\n",
       " 34        3606  2022-12-21 07  264.50\n",
       " 35        3538  2022-12-21 08  263.35\n",
       " 36        3483  2022-12-21 09  263.15\n",
       " 37        3317  2022-12-21 10  261.00\n",
       " 38        3199  2022-12-21 11  260.11)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path=f\"D:\\Documents\\FYP\\Daily Data Cleaned New (Hours)\\ENGRO.csv\"\n",
    "pd.read_csv(path).head(),pd.read_csv(path).tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training long term models without training split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Preparing data from taining i.e. changing sequancial data <br> into 100 time steps (100 columns for each elements/row)\n",
    "* Defining structure of stacked LSTM nueral network with <br> 3 layers, each containing 50 neurons\n",
    "* Trainig models on historical/long term data (Jan 2018 to June 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGRO Trained and saved successfully.\n",
      "ATRL Trained and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "list_companies=['ENGRO','LUCK', 'OGDC','FFC','HBL','HUBC','PPL',\n",
    "                'POL','EFERT', 'MCB','UBL','DGKC','PSO','SEARL','MLCF',\n",
    "                'BAHL','MARI','TRG','ATRL','UNITY','SYS','MEBL','GHNI',\n",
    "                'PIOC','CHCC','PAEL','ISL','KAPCO']\n",
    "\n",
    "for item  in list_companies:\n",
    "    path=f\"Historical Data/{item}.csv\"\n",
    "    data=pd.read_csv(path, index_col='TIME', parse_dates=True)\n",
    "    data=data.sort_values(by='TIME')\n",
    "    feat_data=data['HIGH']\n",
    "    scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    \n",
    "    scaled_feat_data=scaler.fit_transform(np.array(feat_data).reshape(-1,1))\n",
    "    def create_dataset(dataset, time_step=1):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-time_step-1):\n",
    "            a = dataset[i:(i+time_step), 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + time_step, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "    time_step = 100\n",
    "    X_train, y_train = create_dataset(scaled_feat_data, time_step)\n",
    "    X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "    model=Sequential()\n",
    "    model.add(layers.LSTM(50,return_sequences=True,input_shape=(100,1)))\n",
    "    model.add(layers.LSTM(50,return_sequences=True))\n",
    "    model.add(layers.LSTM(50))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(loss='mean_absolute_error',optimizer='adam',metrics=[\"mae\"])\n",
    "    trained_LSTM_Model=model.fit(X_train,y_train,epochs=100,batch_size=64,verbose=0)\n",
    "    path=f\"{item}.h5\"\n",
    "    model.save(path)\n",
    "    print(f\"{item} Trained and saved successfully.\")\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (RMSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluating trained models with testing data\n",
    "* Checkng  Root Mean Squared Error \n",
    "* Testing data (July 2022 to Dec 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGRO Testing RMSE= 0.09593436377549973\n",
      "LUCK Testing RMSE= 0.07090905006224886\n",
      "OGDC Testing RMSE= 0.0736011275619031\n",
      "FFC Testing RMSE= 0.07177131337622321\n",
      "HBL Testing RMSE= 0.023819031200247046\n",
      "HUBC Testing RMSE= 0.06802470341475683\n",
      "PPL Testing RMSE= 0.08677431774619589\n",
      "POL Testing RMSE= 0.056670847258128085\n",
      "EFERT Testing RMSE= 0.04978023177476003\n",
      "MCB Testing RMSE= 0.04963986582754888\n",
      "UBL Testing RMSE= 0.10770490528840722\n",
      "DGKC Testing RMSE= 0.07689685095234002\n",
      "PSO Testing RMSE= 0.055381038625196964\n",
      "SEARL Testing RMSE= 0.03924445432885445\n",
      "MLCF Testing RMSE= 0.08163775345629895\n",
      "BAHL Testing RMSE= 0.12103945277319282\n",
      "MARI Testing RMSE= 0.0739463368330766\n",
      "TRG Testing RMSE= 0.05642160666862987\n",
      "ATRL Testing RMSE= 0.07820734921837831\n",
      "UNITY Testing RMSE= 0.04548630156525249\n",
      "SYS Testing RMSE= 0.0792659160161357\n",
      "MEBL Testing RMSE= 0.0423957335338384\n",
      "GHNI Testing RMSE= 0.0676795066165442\n",
      "PIOC Testing RMSE= 0.07273333745103971\n",
      "CHCC Testing RMSE= 0.06039967946633148\n",
      "PAEL Testing RMSE= 0.054341845814996126\n",
      "ISL Testing RMSE= 0.05723561803361117\n",
      "KAPCO Testing RMSE= 0.07291275747815534\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras import layers\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "list_companies=['ENGRO','LUCK', 'OGDC','FFC','HBL','HUBC','PPL',\n",
    "                'POL','EFERT', 'MCB','UBL','DGKC','PSO','SEARL',\n",
    "                'MLCF','BAHL','MARI','TRG','ATRL','UNITY','SYS',\n",
    "                'MEBL','GHNI','PIOC','CHCC','PAEL','ISL','KAPCO']\n",
    "\n",
    "for item  in list_companies:\n",
    "    path=f\"D:\\Documents\\FYP\\Historical Data (dec)\\{item}.csv\"\n",
    "    data=pd.read_csv(path, index_col='TIME', parse_dates=True)\n",
    "    data=data.sort_values(by='TIME')\n",
    "    feat_data=data['HIGH']\n",
    "    scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_feat_data=scaler.fit_transform(np.array(feat_data).reshape(-1,1))\n",
    "    def create_dataset(dataset, time_step=1):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-time_step-1):\n",
    "            a = dataset[i:(i+time_step), 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + time_step, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "    time_step = 100\n",
    "    X_test, ytest = create_dataset(scaled_feat_data ,time_step)\n",
    "    X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "    path=f\"D:\\Documents\\FYP\\Trained Models\\Historical\\Kaggle\\HIGH\\{item}.h5\"\n",
    "    model=load_model(path)\n",
    "    test_predict=model.predict(X_test,verbose=0)\n",
    "    print(f\"{item} Testing RMSE= {math.sqrt(mean_squared_error(ytest,test_predict))}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting long term data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Preparing data from prediction i.e. changing sequancial <br> data into 100 time steps (100 columns for each elements/row)\n",
    "* Predicting data for future 10 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{1: 268.6}, {2: 267.9}, {3: 267.6}, {4: 267.3}, {5: 267.1}, {6: 266.8}, {7: 266.5}, {8: 266.2}, {9: 265.8}, {10: 265.3}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def getLongTermPredictions(days,company,variable):\n",
    "    lst_output=[]\n",
    "    predicted=[]\n",
    "    n_steps=100\n",
    "    path=f\"D:\\Documents\\FYP\\Historical Data New (sorted)\\{company}.csv\"\n",
    "    data=pd.read_csv(path, index_col='TIME', parse_dates=True)\n",
    "    data=data.sort_values(by='TIME')\n",
    "    feat_data=data[variable][:100]\n",
    "    scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    test_data=scaler.fit_transform(np.array(feat_data).reshape(-1,1))\n",
    "    path=f\"D:\\Documents\\FYP\\Trained Models\\Historical\\Kaggle\\{variable}\\{company}.h5\"\n",
    "    model=load_model(path, compile=False)\n",
    "    x_input=test_data[len(test_data)-100:].reshape(1,-1)\n",
    "    temp_input=list(x_input)\n",
    "    temp_input=temp_input[0].tolist()\n",
    "    for i in range(days):\n",
    "        if(len(temp_input)>100):\n",
    "            x_input=np.array(temp_input[1:])\n",
    "            x_input=x_input.reshape(1,-1)\n",
    "            x_input = x_input.reshape((1, n_steps, 1))\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "            temp_input=temp_input[1:]\n",
    "            lst_output.extend(yhat.tolist())\n",
    "        else:\n",
    "            x_input = x_input.reshape((1, n_steps,1))\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "            lst_output.extend(yhat.tolist())\n",
    "        predicted.append({i+1:round(float(scaler.inverse_transform([lst_output[-1]])),1)})\n",
    "    return predicted\n",
    "print(getLongTermPredictions(10,'ENGRO','HIGH'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original vs Predicted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* comparing original/actual data and predicted data manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TIME\n",
       " 2022-11-28    265.20\n",
       " 2022-11-29    268.99\n",
       " 2022-11-30    264.49\n",
       " 2022-12-01    267.90\n",
       " 2022-12-02    268.10\n",
       " 2022-12-05    267.25\n",
       " 2022-12-06    263.89\n",
       " 2022-12-07    264.95\n",
       " 2022-12-08    264.25\n",
       " 2022-12-09    265.34\n",
       " Name: HIGH, dtype: float64,\n",
       " [{1: 268.6},\n",
       "  {2: 267.9},\n",
       "  {3: 267.6},\n",
       "  {4: 267.3},\n",
       "  {5: 267.1},\n",
       "  {6: 266.8},\n",
       "  {7: 266.5},\n",
       "  {8: 266.2},\n",
       "  {9: 265.8},\n",
       "  {10: 265.3}])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "company=\"ENGRO\"\n",
    "path=f\"D:\\Documents\\FYP\\Historical Data New (sorted)/{company}.csv\"\n",
    "data=pd.read_csv(path, index_col='TIME', parse_dates=True)\n",
    "data=data.sort_values(by='TIME')\n",
    "feat_data=data['HIGH']\n",
    "feat_data[100:],getLongTermPredictions(10,company,'HIGH')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training short term models without testing split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Preparing data from taining i.e. changing sequancial data <br> into 25 time steps (25 columns for each elements)\n",
    "* Defining structure of stacked LSTM nueral network with <br> 3 layers, each containing 50 neurons\n",
    "* Trainig models on Daily/short term data (07 July 2022 to 09 Dec 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGRO Trained and saved successfully.\n",
      "ATRL Trained and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "list_companies=['ENGRO','LUCK', 'OGDC','FFC','HBL','HUBC','PPL',\n",
    "                'POL','EFERT', 'MCB','UBL','DGKC','PSO','SEARL',\n",
    "                'MLCF','BAHL','MARI','TRG','ATRL','UNITY','SYS',\n",
    "                'MEBL','GHNI','PIOC','CHCC','PAEL','ISL','KAPCO']\n",
    "\n",
    "for item  in list_companies:\n",
    "    path=f\"D:\\Documents\\FYP\\Daily Data Cleaned (Hours)\\{item}.csv\"\n",
    "    data=pd.read_csv(path, index_col='Date', parse_dates=True)\n",
    "    data=data.sort_values(by='Date')\n",
    "    feat_data=data['Value']\n",
    "    scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_feat_data=scaler.fit_transform(np.array(feat_data).reshape(-1,1))\n",
    "    def create_dataset(dataset, time_step=1):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-time_step-1):\n",
    "            a = dataset[i:(i+time_step), 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + time_step, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "    time_step = 25\n",
    "    X_train, y_train = create_dataset(scaled_feat_data, time_step)\n",
    "    X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "    model=Sequential()\n",
    "    model.add(layers.LSTM(50,return_sequences=True))\n",
    "    model.add(layers.LSTM(50,return_sequences=True))\n",
    "    model.add(layers.LSTM(50))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam',metrics=[\"mse\"])\n",
    "    trained_LSTM_Model=model.fit(X_train,y_train,epochs=100,batch_size=64,verbose=0)\n",
    "    path=f\"D:\\Documents\\FYP\\Trained Models\\Daily\\{item}.h5\"\n",
    "    model.save(path)\n",
    "    print(f\"{item} Trained and saved successfully.\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (RMSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluating trained models with testing data\n",
    "* Checkng  Root Mean Squared Error \n",
    "* Testing data (12 Dec 2022 to 21 Dec 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGRO Testing RMSE= 0.10048811280406324\n",
      "LUCK Testing RMSE= 0.13961047805482424\n",
      "OGDC Testing RMSE= 0.1663619234805757\n",
      "FFC Testing RMSE= 0.21430400306733294\n",
      "HBL Testing RMSE= 0.15295335132192325\n",
      "HUBC Testing RMSE= 0.06568707546415622\n",
      "PPL Testing RMSE= 0.16301878154998667\n",
      "POL Testing RMSE= 0.16097892825045115\n",
      "EFERT Testing RMSE= 0.15757405078852452\n",
      "MCB Testing RMSE= 0.1215592059594386\n",
      "DGKC Testing RMSE= 0.21679293010707215\n",
      "PSO Testing RMSE= 0.10621913550892488\n",
      "UNITY Testing RMSE= 0.10966841934886817\n",
      "SYS Testing RMSE= 0.2912707218925379\n",
      "MEBL Testing RMSE= 0.2790738318314404\n",
      "GHNI Testing RMSE= 0.0531345750138595\n",
      "PIOC Testing RMSE= 0.0899265135616821\n",
      "CHCC Testing RMSE= 0.2345220253557714\n",
      "PAEL Testing RMSE= 0.12097795665895905\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras import layers\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "list_companies = ['ENGRO', 'LUCK', 'OGDC', 'FFC', 'HBL', 'HUBC',\n",
    "                  'PPL', 'POL', 'EFERT', 'MCB', 'DGKC', 'PSO', 'UNITY',\n",
    "                  'SYS', 'MEBL', 'GHNI', 'PIOC', 'CHCC', 'PAEL']\n",
    "time_step = 25\n",
    "for item  in list_companies:\n",
    "    path=f\"D:\\Documents\\FYP\\Daily Data Cleaned New (Hours)\\{item}.csv\"\n",
    "    data=pd.read_csv(path)\n",
    "    data=data.sort_values(by='Date')\n",
    "    feat_data=data['Value']\n",
    "    scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_feat_data=scaler.fit_transform(np.array(feat_data).reshape(-1,1))\n",
    "    def create_dataset(dataset, time_step=1):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-time_step-1):\n",
    "            a = dataset[i:(i+time_step), 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + time_step, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "    X_test, ytest = create_dataset(scaled_feat_data ,time_step)\n",
    "    X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "    path=f\"D:\\Documents\\FYP\\Trained Models\\Daily\\{item}.h5\"\n",
    "    model=load_model(path)\n",
    "    test_predict=model.predict(X_test,verbose=0)\n",
    "    print(f\"{item} Testing RMSE= {math.sqrt(mean_squared_error(ytest,test_predict))}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting short term data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Preparing data from prediction i.e. changing sequancial <br> data into 100 time steps (25 columns for each elements/row)\n",
    "* Predicting data for future 8 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: 261.3},\n",
       " {2: 261.0},\n",
       " {3: 260.9},\n",
       " {4: 261.0},\n",
       " {5: 261.0},\n",
       " {6: 261.1},\n",
       " {7: 261.1},\n",
       " {8: 261.1}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def getShortTermPredictions(days,company):\n",
    "    lst_output=[]\n",
    "    predicted=[]\n",
    "    n_steps=25\n",
    "    path=f\"D:\\Documents\\FYP\\Daily Data Cleaned New (Hours)\\{company}.csv\"\n",
    "    data=pd.read_csv(path, index_col='Date', parse_dates=True)\n",
    "    data=data.sort_values(by='Date')\n",
    "    feat_data=data[\"Value\"][-25:]\n",
    "    scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    test_data=scaler.fit_transform(np.array(feat_data).reshape(-1,1))\n",
    "    path=f\"D:\\Documents\\FYP\\Trained Models\\Daily\\{company}.h5\"\n",
    "    model=load_model(path)\n",
    "    x_input=test_data[len(test_data)-25:].reshape(1,-1)\n",
    "    temp_input=list(x_input)\n",
    "    temp_input=temp_input[0].tolist()\n",
    "    for i in range(days):\n",
    "        if(len(temp_input)>25):\n",
    "            x_input=np.array(temp_input[1:])\n",
    "            x_input=x_input.reshape(1,-1)\n",
    "            x_input = x_input.reshape((1, n_steps, 1))\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "            temp_input=temp_input[1:]\n",
    "            lst_output.extend(yhat.tolist())\n",
    "        else:\n",
    "            x_input = x_input.reshape((1, n_steps,1))\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "            lst_output.extend(yhat.tolist())\n",
    "        predicted.append({i+1:round(float(scaler.inverse_transform([lst_output[-1]])),1)})\n",
    "    return predicted\n",
    "getShortTermPredictions(8,\"ENGRO\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original vs Predicted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* comparing original/actual data and predicted data manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Date\n",
       " 2022-12-21 04:00:00    264.90\n",
       " 2022-12-21 05:00:00    266.01\n",
       " 2022-12-21 06:00:00    264.97\n",
       " 2022-12-21 07:00:00    264.50\n",
       " 2022-12-21 08:00:00    263.35\n",
       " 2022-12-21 09:00:00    263.15\n",
       " 2022-12-21 10:00:00    261.00\n",
       " 2022-12-21 11:00:00    260.11\n",
       " Name: Value, dtype: float64,\n",
       " [{1: 261.3},\n",
       "  {2: 261.0},\n",
       "  {3: 260.9},\n",
       "  {4: 261.0},\n",
       "  {5: 261.0},\n",
       "  {6: 261.1},\n",
       "  {7: 261.1},\n",
       "  {8: 261.1}])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "company=\"ENGRO\"\n",
    "path=f\"D:\\Documents\\FYP\\Daily Data Cleaned New (Hours)\\{company}.csv\"\n",
    "data=pd.read_csv(path, index_col='Date', parse_dates=True)\n",
    "data=data.sort_values(by='Date')\n",
    "feat_data=data['Value']\n",
    "feat_data[-8:],getShortTermPredictions(8,company)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d654ff688aff3455eedc780c8d45a047d854fe40ade0b99e326e6e0639f14401"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
